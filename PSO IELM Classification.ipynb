{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.linalg import pinv2\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import linalg as LA\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pyswarms as ps\n",
    "\n",
    "from pyswarms.single.global_best import GlobalBestPSO\n",
    "from pyswarms.utils.functions import single_obj as fx\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function uses parameters like radius, ditance between two moons, number of samples in total and width as per which\n",
    "#half moon will be generated and the data points will be stored along with their labels.\n",
    "\n",
    "features=3 #number of attributes +label column\n",
    "instances=3000\n",
    "r=10\n",
    "d=-4\n",
    "w=4\n",
    "\n",
    "\n",
    "\n",
    "#First we need to check whether the number of samples are even or not\n",
    "if (instances%2!=0) :\n",
    "    print(\"*****Error****** Number of samples are not valid; They should be even \")\n",
    "    instances=instances+1\n",
    "\n",
    "#Matrix initialization of samples with 0 as initial value for x,y and label values.\n",
    "valuesofSamples=np.zeros((features,instances),dtype=int)\n",
    "#print(valuesofSamples)\n",
    "\n",
    "# Boundary condition checking\n",
    "if (r<w/2):\n",
    "    print(\"*****Error******  Radius is not enough\")\n",
    "\n",
    "#Creating random float values of x and y coordinates of half the instances\n",
    "randomval=np.random.random((2,int(instances/2)))\n",
    "#print (randomval)\n",
    "\n",
    "radii=(r-w/2)+w*randomval[0][:]\n",
    "\n",
    "#Calculating outer radius for one half moon\n",
    "theta=np.pi*randomval[1][:]\n",
    "\n",
    "#Creation of datasets for both the half moons\n",
    "x_class1=np.multiply(radii,np.cos(theta)) #X coordinate for 1st half data points\n",
    "y_class1=np.multiply(radii,np.sin(theta)) #y coordinate for 1st half data points\n",
    "label_class1=np.ones((1,len(x_class1)),dtype=int) #providing label as 1 to the entire 1st half moon data\n",
    "label_class1=np.hstack(label_class1)\n",
    "                     \n",
    "x_class2=np.multiply(radii,np.cos(-theta))+r #X coordinate for 2nd half data points\n",
    "y_class2=np.multiply(radii,np.sin(-theta))-d #y coordinate for 2nd half data points\n",
    "label_class2=0*np.ones((1,len(x_class2)),dtype=int) #providing label as 0 to the 2nd half moon data\n",
    "label_class2=np.hstack(label_class2)\n",
    "                     \n",
    "\n",
    "\n",
    "#Now we will create a single matrix with all the x,y coordinates of all the points belonging to both the halves\n",
    "#using a nested list functionality,with their corresponding labels\n",
    "valuesofSamples[0,:]=np.concatenate([x_class1,x_class2])\n",
    "valuesofSamples[1,:]=np.concatenate([y_class1,y_class2])\n",
    "\n",
    "valuesofSamples[2,:]=np.concatenate(([label_class1,label_class2]))\n",
    "\n",
    "#converting to dataframe and Transposing it to get columns on the top\n",
    "df=(pd.DataFrame(valuesofSamples)).T\n",
    "\n",
    "DF=df.rename(columns={0:'x',1:'y',2:'labels'}) #Renaming the column name as per index\n",
    "\n",
    "\n",
    "DF.sample(frac=1) # Randomizing the whole dataset\n",
    "DF_train,DF_test=train_test_split(DF, test_size=0.30) #splitting the dataset with 1000 training and 2000 testing data points\n",
    "#Separating the feature data and label data for the training set\n",
    "feature_valTrain=DF_train[['x','y']] \n",
    "Label_train=pd.DataFrame(DF_train['labels'])\n",
    "\n",
    "#Separating the feature data and label data for the testing set        \n",
    "feature_valTest=DF_test[['x','y']]\n",
    "Label_test=pd.DataFrame(DF_test['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19047619 0.81818182]\n"
     ]
    }
   ],
   "source": [
    "#Scaling Train and test features\n",
    "scale=MaxAbsScaler()\n",
    "Train_features=scale.fit_transform(feature_valTrain)\n",
    "Test_features=scale.fit_transform(feature_valTest)\n",
    "print(Train_features[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing one hot encoding on labels to convert classes to a vector notation or binarizing the labels\n",
    "encoder=OneHotEncoder(categories='auto')\n",
    "\n",
    "Train_Label=encoder.fit_transform(Label_train).toarray()\n",
    "Test_Label=encoder.fit_transform(Label_test).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation function definintion\n",
    "def sigmoid(y):\n",
    "    a = 1/(1+np.exp(-y))\n",
    "    #a=np.maximum(0,y)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy using the maximum index of the binary value and comparing that with the original label index\n",
    "def accuracy(Error,target):\n",
    "    count=0\n",
    " \n",
    "    for i in range(len(Error)):\n",
    "        \n",
    "        index=np.argmax(target[i])    # getting the index of the target label\n",
    "        index_predicted=np.argmax(Error[i]) #getting the index of the predicted label \n",
    "        if index_predicted==index:\n",
    "            count=count+1\n",
    "    predicted_accuracy=(count/len(Error))*100   #calculating the accuracy based on the total correct predictions vs total values\n",
    "    #print(predicted_accuracy)\n",
    "    return predicted_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining ielm objective function to get the minimum loss which is been compared using Global best FUNCTION OF sWARM OPTIMIZER\n",
    "# THIS FUNCTION IS CALLED BY EACH PARTICLE WHICH RECEIVES THE LOSS AND THEN THAT LOSS IS KEPT IN A LIST WHICH IS THEN\n",
    "#COMPARED TO GET THE MINIMUM LOSS AND HENCE THE OPTIMIZED WEIGHTS AND BIASES.\n",
    "#here loss means maximizing the accuracy. hen 1- accuracy to get the loss\n",
    "\n",
    "def IELm_objective(dimensions,E):\n",
    "  \n",
    "    a = dimensions[0:2].reshape((len(Train_features[0]),1)) #retrieving weight which is first 2 elements of the array dimensions\n",
    "    b=dimensions[-1].reshape(1,1)\n",
    "    predicted_accuracy=0  #Initial value of accuracy\n",
    "    beta=0\n",
    "    mul=np.dot(Train_features,a)\n",
    "    sum=mul+b\n",
    "    activation=sigmoid(sum) # activation function sigmoid to calculate H\n",
    "    beta_overall=np.dot(np.linalg.pinv(activation),E) # beta= inverse(H) * Target Label. As our target keeps on changing with new neuron\n",
    "                                         #hence E\n",
    "    y=np.dot(activation,beta_overall) # Calculated output\n",
    "    E=E-y # Updated Training Label or Error\n",
    "    predicted_accuracy=accuracy(y,Train_Label) #Calculating the accuracy of the predicted labels\n",
    "    return(1-predicted_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterating the number of particles and calling the objctive function to get the loss.\n",
    "def swarm_initializer(input,**kwargs):\n",
    "\n",
    "    particles = input.shape[0]\n",
    "    for i in range(particles):\n",
    "        loss = IELm_objective(input[i],**kwargs) #passing the number of values required after optimization and updated target variable via **kwargs which is updated target label only\n",
    "    return np.array(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-30 22:02:51,031 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best:   0%|                                                                                |0/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- TRAINING ACCURACY ----------\n",
      "\n",
      "49.714285714285715 With 1  Neuron\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=-49.3\n",
      "2020-10-30 22:02:59,501 - pyswarms.single.global_best - INFO - Optimization finished | best cost: -49.28571428571429, best pos: [0.96129926 0.43586493 0.09866023]\n",
      "2020-10-30 22:02:59,512 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best:   2%|█▎                                                             |1/50, best_cost=-49.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-49.28571428571429\n",
      "[0.96129926 0.43586493 0.09866023]\n",
      "\n",
      "50.28571428571429 with 2  Neuron \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=-49.3\n",
      "2020-10-30 22:03:05,525 - pyswarms.single.global_best - INFO - Optimization finished | best cost: -49.28571428571429, best pos: [-0.09939794  0.55941817  0.20307257]\n",
      "2020-10-30 22:03:05,538 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best:   2%|█▎                                                             |1/50, best_cost=-49.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-49.28571428571429\n",
      "[-0.09939794  0.55941817  0.20307257]\n",
      "\n",
      "50.28571428571429 with 3  Neuron \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=-49.3\n",
      "2020-10-30 22:03:11,436 - pyswarms.single.global_best - INFO - Optimization finished | best cost: -49.28571428571429, best pos: [0.95496218 0.00378372 0.34678914]\n",
      "2020-10-30 22:03:11,449 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best:   2%|█▎                                                             |1/50, best_cost=-48.7"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-49.28571428571429\n",
      "[0.95496218 0.00378372 0.34678914]\n",
      "\n",
      "49.714285714285715 with 4  Neuron \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=-49.3\n",
      "2020-10-30 22:03:17,205 - pyswarms.single.global_best - INFO - Optimization finished | best cost: -49.28571428571429, best pos: [ 0.89891057  0.09168554 -0.30484746]\n",
      "2020-10-30 22:03:17,220 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best:   2%|█▎                                                             |1/50, best_cost=-49.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-49.28571428571429\n",
      "[ 0.89891057  0.09168554 -0.30484746]\n",
      "\n",
      "49.714285714285715 with 5  Neuron \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=-49.3\n",
      "2020-10-30 22:03:22,859 - pyswarms.single.global_best - INFO - Optimization finished | best cost: -49.28571428571429, best pos: [ 0.62093105  0.26863987 -0.15717763]\n",
      "2020-10-30 22:03:22,875 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best:   2%|█▎                                                             |1/50, best_cost=-49.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-49.28571428571429\n",
      "[ 0.62093105  0.26863987 -0.15717763]\n",
      "\n",
      "50.28571428571429 with 6  Neuron \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=-49.3\n",
      "2020-10-30 22:03:29,021 - pyswarms.single.global_best - INFO - Optimization finished | best cost: -49.28571428571429, best pos: [-0.36633787  0.08672393 -0.5779917 ]\n",
      "2020-10-30 22:03:29,036 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best:   2%|█▎                                                             |1/50, best_cost=-48.7"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-49.28571428571429\n",
      "[-0.36633787  0.08672393 -0.5779917 ]\n",
      "\n",
      "50.28571428571429 with 7  Neuron \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=-48.7\n",
      "2020-10-30 22:03:34,952 - pyswarms.single.global_best - INFO - Optimization finished | best cost: -48.714285714285715, best pos: [-0.33066761 -0.70556427  0.75334801]\n",
      "2020-10-30 22:03:34,964 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best:   2%|█▎                                                             |1/50, best_cost=-48.7"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-48.714285714285715\n",
      "[-0.33066761 -0.70556427  0.75334801]\n",
      "\n",
      "49.714285714285715 with 8  Neuron \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=-49.3\n",
      "2020-10-30 22:03:40,892 - pyswarms.single.global_best - INFO - Optimization finished | best cost: -49.28571428571429, best pos: [-0.30815906 -0.91840134  0.70201206]\n",
      "2020-10-30 22:03:40,906 - pyswarms.single.global_best - INFO - Optimize for 50 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best:   2%|█▎                                                             |1/50, best_cost=-49.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-49.28571428571429\n",
      "[-0.30815906 -0.91840134  0.70201206]\n",
      "\n",
      "49.714285714285715 with 9  Neuron \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████|50/50, best_cost=-49.3\n",
      "2020-10-30 22:03:47,053 - pyswarms.single.global_best - INFO - Optimization finished | best cost: -49.28571428571429, best pos: [-0.63682437 -0.04874794  0.95756228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-49.28571428571429\n",
      "[-0.63682437 -0.04874794  0.95756228]\n",
      "\n",
      "50.28571428571429 with 10  Neuron \n",
      "\n",
      "TRAINING TIME 56.053\n"
     ]
    }
   ],
   "source": [
    "L=0 # Number of neurons in hidden layer inititally\n",
    "max_neuron=10 # Maximum number of Neurons \n",
    "Accuracy_expected=97 # Expected accuracy or a threshold value that the network should have to break stop adding the neurons\n",
    "predicted_accuracy=0  #Initial value of accuracy\n",
    "beta=0\n",
    "E=Train_Label\n",
    " #Initial value of error as training set label, to keep a check on the differnce between the target and the predicted\n",
    "new_a=np.random.uniform(low=-1,high=1,size=[len(Train_features[0]),1]) #Generating numpy array of weights for the first hidden neuron\n",
    "new_b=np.random.uniform(low=-1,high=1,size=[1]) #Generating numpy array of bias for the first hidden neuron\n",
    "\n",
    "# ****** TRAININIG STARTS HERE ***********\n",
    "#Loop until the hidden layer reaches maximum number of neurons and until the predicted accuracy doesnt surpass the expected accuracy\n",
    "start_time=time.time()\n",
    "options = {'c1': 0.5, 'c2': 0.3, 'w':0.9} #Setting the hyper parameter and passing to the optimizer\n",
    "lb = [-1,-1,-1] #setting the upper and lower limit bounds for the weights and biases\n",
    "ub = [1,1,1]\n",
    "bounds=(lb,ub)\n",
    "while L<max_neuron and predicted_accuracy<Accuracy_expected:   \n",
    "    L+=1\n",
    "    \n",
    "    #for the first neuron using the random produced weights and biases followed by further steps\n",
    "    if L==1:    \n",
    "        mul=np.dot(Train_features,new_a)\n",
    "        sum=mul+new_b\n",
    "        activation=sigmoid(sum) # activation function sigmoid to calculate H\n",
    "        beta_overall=np.dot(np.linalg.pinv(activation),E) # beta= inverse(H) * Target Label. As our target keeps on changing with new neuron\n",
    "                                             #hence E\n",
    "        y=np.dot(activation,beta_overall) # Calculated output\n",
    "        E=E-y # Updated Training Label or Error\n",
    "        predicted_accuracy=accuracy(y,Train_Label) #Calculating the accuracy of the predicted labels\n",
    "        print(\"-------- TRAINING ACCURACY ----------\")\n",
    "        print(\"\")\n",
    "        print(predicted_accuracy,\"With\", L,\" Neuron\")\n",
    "        \n",
    "    else:\n",
    "        optimizer = ps.single.GlobalBestPSO(n_particles=20,dimensions=3, options=options, bounds=bounds) #setting number of particles 20 with bounds as [-1,1] for both weights and bias\n",
    "        cost, weights_bias = optimizer.optimize(swarm_initializer, iters=50,E=E) #calling the iterating function of particles which will run the objective functioin and also passing updated labels for calulcaing the error \n",
    "        print(cost)\n",
    "        print(weights_bias)\n",
    "\n",
    "        a=weights_bias[:2].reshape(len(Train_features[0]),1)\n",
    "#         a=np.random.uniform(low=-1,high=1,size=[len(Train_features[0]),1]) #creating a new random weights for the second neuron and so on\n",
    "#         b=np.random.uniform(low=-1,high=1,size=[1]) #creating a new random bias for the second neuron and so on\n",
    "        b=weights_bias[-1].reshape(1,)\n",
    "        new_a=np.append(new_a,a,axis=1) # adding the newly created weight to the weight matrix of the previous neuron\n",
    "        mul=np.dot(Train_features,a) \n",
    "        new_b=np.append(new_b,b,axis=0) # adding the newly created bias to the bias matrix of the previous neuron\n",
    "        sum=mul+b\n",
    "        activation=sigmoid(sum)\n",
    "        beta=np.dot(pinv2(activation), E)\n",
    "        y=np.dot(activation,beta)\n",
    "        E=E-y\n",
    "        print(\"\")\n",
    "        beta_overall=np.append(beta_overall,beta,axis=0)\n",
    "        predicted_accuracy=accuracy(y,Train_Label)\n",
    "        print(predicted_accuracy,\"with\", L,\" Neuron \")\n",
    "end_time=round(time.time()-start_time,3)\n",
    "print(\"\")\n",
    "print(\"TRAINING TIME\",end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- TESTING ACCURACY --------\n",
      "\n",
      "84.33333333333334 with 10  neurons \n",
      "\n",
      "TESTING TIME 0.004\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "print(\"-------- TESTING ACCURACY --------\")\n",
    "print(\"\")\n",
    "start_time=time.time()\n",
    "h_test=np.dot(Test_features,new_a) #using same weights, bias and beta produced during training phase\n",
    "sum_test=h_test+new_b\n",
    "activation_test=sigmoid(sum_test)\n",
    "y_test=np.dot(activation_test,beta_overall)\n",
    "testing_accuracy=accuracy(y_test,Test_Label)\n",
    "\n",
    "print(testing_accuracy,\"with\", L,\" neurons \")\n",
    "end_time=round(time.time()-start_time,3)\n",
    "print(\"\")\n",
    "print(\"TESTING TIME\",end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
